{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "《深入理解 Tensorflow 架构设计与实现原理》第二章， Tensorflow 数据处理方法\n",
    "- 输入数据集\n",
    "- 模型参数\n",
    "- 命令行参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输入数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  使用Tensorflow input pipeline 读取\n",
    "- 创建文件名列表  \n",
    "- 创建文件名队列  \n",
    "- 创建 Reader 和 Decoder  \n",
    "- 创建样例队列  \n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/4685306-eae218123ff6abd2.gif?imageMogr2/auto-orient/strip%7CimageView2/2/w/700)\n",
    "#### 创建文件名列表\n",
    "- 使用 python list　　\n",
    "- 使用 ```tf.train.match_filenames_once``` 返回一个存储了符合该匹配模式的文件名列表变量  \n",
    "```python\n",
    "def match_filenames_once(pattern, name=None):\n",
    "  \"\"\"Save the list of files matching pattern, so it is only computed once.\n",
    "  NOTE: The order of the files returned can be non-deterministic.\n",
    "  Args:\n",
    "    pattern: A file pattern (glob), or 1D tensor of file patterns.\n",
    "    name: A name for the operations (optional).\n",
    "  Returns:\n",
    "    A variable that is initialized to the list of files matching the pattern(s).\n",
    "```\n",
    "\n",
    "#### 创建文件名队列\n",
    "- 使用 ```tf.train.string_input_producer()``` 遍历整个数据集，输入是前面创建的文件名列表，因此其参数 num_epochs 表示训练的epoch数\n",
    "\n",
    "```python\n",
    "def string_input_producer(string_tensor,\n",
    "                          num_epochs=None,\n",
    "                          shuffle=True,\n",
    "                          seed=None,\n",
    "                          capacity=32,\n",
    "                          shared_name=None,\n",
    "                          name=None,\n",
    "                          cancel_op=None):\n",
    "    \"\"\"\n",
    "    - string_tensor 存储文件名列表的字符串张量， A 1-D string tensor with the strings to produce\n",
    "    - num_epochs 训练周期\n",
    "    - shufftle 是否打乱文件名顺序\n",
    "    - capacity 文件名队列容量，在图像处理中用的会比较多吧，毕竟图像是一张一张的，而文本基本上都是存储在一个csv文件中\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "#### 创建 Reader 和 Decoder\n",
    "|文件格式|Reader类型| Decoder类型|  \n",
    "|---|---|---|\n",
    "|CSV文件|tf.TextLineReader|tf.decoder_csv|\n",
    "|TFRecord文件|tf.TFRecordReader|tf.parse_single_example|\n",
    "|自由格式|tf.FixedLengthRecordReader|tf.decoder_raw|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CSV 文件\n",
    "纯文本形式存储，多条记录间以换行符分隔，每条记录由多个字段组成，字段间以制表符或逗号分隔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  file_id,label,api,tid,return_value,index\n",
      "0     0,0,GetSystemTimeAsFileTime,2644,0,0\n",
      "1     0,0,NtAllocateVirtualMemory,2644,0,1\n",
      "2         0,0,NtFreeVirtualMemory,2644,0,2\n",
      "3     0,0,NtAllocateVirtualMemory,2644,0,3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panxie/anaconda3/envs/NLP/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/panxie/anaconda3/envs/NLP/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "### 也可以使用 pandas 分块读取\n",
    "import pandas as pd\n",
    "\n",
    "# 参数 iterator 表示迭代读取\n",
    "reader = pd.read_table('train.csv', sep='\\n', chunksize=4, iterator=False)\n",
    "\n",
    "# print(reader.get_chunk(5))  # pandas.core.frame.DataFrame\n",
    "print(reader.get_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 ```tf.TextLineReader``` 读取数据 https://www.tensorflow.org/api_docs/python/tf/TextLineReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panxie/anaconda3/envs/NLP/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/panxie/anaconda3/envs/NLP/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# 用 tf.TextLineReader \n",
    "# 这里我们数据集只有一个文件 train.csv 所以文件名列表用 python list 表示\n",
    "import tensorflow as tf\n",
    "# 创建文件名队列，因为只有一个文件，所以文件名列表就省了\n",
    "filename_queue = tf.train.string_input_producer([\"train.csv\"])\n",
    "# 创建读取 Reader\n",
    "reader = tf.TextLineReader()\n",
    "# 从文件名队列中读取 CVS 中的一条数据记录 value\n",
    "_, value = reader.read(filename_queue)\n",
    "# 设置数据记录的默认值，当记录中有缺省值时，用默认值填充\n",
    "record_defaults = [[],[],[],[],[],[]]\n",
    "file_id, label, api, tid, return_value, index = tf.decode_csv(value,record_defaults=record_defaults)\n",
    "features = tf.stack([file_id, label, api, tid, return_value, index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了可以用 tf.TextLineReader.read 一次读取一条记录之外，还可以用 tf.TextLineReader.read_up_to()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFRecords 文件\n",
    "存储的是有结构的序列化字符块，它是 Tensorflow 推荐的标准文件格式。\n",
    "\n",
    "###### 使用 tf.python_io.TFRecordWriter 方法将数据保存为 TFRecord 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''writer.py'''\n",
    "# 创建向 TFRecords 文件写数据记录的 writer\n",
    "writer = tf.python_io.TFRecordWriter('stat.tfrecord')\n",
    "# 循环构造输入样例\n",
    "for i in range(1,3):\n",
    "    # 创建 example.proto 中定义的样例\n",
    "    example = tf.train.Example(\n",
    "        features = tf.train.Features(\n",
    "            feature = {\n",
    "            \"id\":tf.train.Feature(int64_List = tf.train.Int64_List(value=[i])),\n",
    "            ...\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    # 将样例序列化为字符串后，写入 stat.TFRecord 文件\n",
    "    writer.write(example.SerializeToString())\n",
    "# 关闭输出流\n",
    "writer.close()\n",
    "\n",
    "### 显然当文件很多的时候可以采用多个 stat.tfrecord 文件来存储"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 使用 tf.TFRecordReader 方法读取 stat.TFRecord 文件中的样例，接着使用 tf.sparse_single_example 将样例转换为张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''reader.py'''\n",
    "# 创建文件名队列\n",
    "filename_queue = tf.train.string_input_producer(['stat.tfrcord'])\n",
    "# 创建读取 TFRecord 文件的 reader\n",
    "reader = tf.TFRecordReader()\n",
    "# 取出 stat.TFRecord 文件中的一条序列化的样例 serialized_example\n",
    "_, serialize_sample = reader.read(filename_queue)  # 返回 key,value\n",
    "# 将一条序列化的样例转换为其包含的所有特征张量\n",
    "features = tf.sparse_single_example(\n",
    "    serialize_sample,\n",
    "    features = {\n",
    "        \"id\":tf.FixedLenFeature([], tf.int64)\n",
    "        ...\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 自由格式文件\n",
    "用户自定义的二进制文件。它的存储对象是字符串，每条记录是一个固定长度的字节块。因此，首先使用 ```tf.FixedLengthRecordReader``` 读取二进制文件中固定长度的字节块，然后使用 ```tf.decode_raw``` 方法将读取的字符串转换为 unit8 类型的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 创建样例队列\n",
    "通过 Reader 和 Decoder 我们得到了特征张量 features，然后在会话执行时，我们需要使用 ```tf.train.start_queue_runners``` 方法启动入队操作的所有线程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接着 TFRecord 的reader 文件\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "# 启动执行入队操作的后台线程\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "# 读取第一条数据\n",
    "for i in range(1,2):\n",
    "    example = sess.run(features)\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种方法容错行很差，因为在队列操作后台线程的生命周期“无人管理”，任何线程出现错误，都会出现异常，导致程序崩溃。\n",
    "\n",
    "常见的是文件名或样例队列越界抛出的 ```tf.errors.OutOfRangeError```. 队列越界通常的原因是读取数据记录的数量超过了 ```tf.train.string_input_producer``` 方法中 num_epochs 数据集便利次数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了处理这种异常，我们使用 ```tf.train.Coordinator``` 方法管理多线程生命周期的协调器。当某个线程出现异常时，它的 should_stop 成员返回 True， for循环结束。然后程序执行 finally 中协调器的 request_stop 成员方法，请求所有线程安全退出。具体 code 在之后例子中使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFRecords 样例数据结构\n",
    "样例和特征的层次结构。Features 由形如 <string, Feature> 的feature字典组成。其中 feature字典的值的数据结构为 Feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建批样例数据的方法\n",
    "前面的最后，我们得到了很多样例，但是这些样例要打包聚合成批数据才能提供模型训练、评估和推理使用。Tensorflow 提供的 ```tf.train.shuffle_batch``` 方法能够使用样例建立批数据，而且能够在打包过程中打乱样例顺序。\n",
    "```python\n",
    "tf.train.shuffle_batch(\n",
    "    tensors,             # 列表或字典，The list or dictionary of tensors to enqueue.\n",
    "    batch_size,   \n",
    "    capacity,            # 样例队列中总的样例个数，最好设置为：min_after_queue + (num_threads + margin)*batch_size\n",
    "    min_after_dequeue,   # 样例队列中出队的样例个数\n",
    "    num_threads=1,\n",
    "    seed=None,\n",
    "    enqueue_many=False,\n",
    "    shapes=None,\n",
    "    allow_smaller_final_batch=False,\n",
    "    shared_name=None,\n",
    "    name=None\n",
    ")\n",
    "```\n",
    "\n",
    "具体参数设置参考：https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/python/training/input.py#L1204\n",
    "\n",
    "以及后续的使用中去进一步了解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 这里是完整输入流水线的伪代码\n",
    "def get_my_example(filename_queue):\n",
    "    reader = tf.SomeRead()            # csv 使用 tf.TextLineReader, TFRecord 使用 tf.TFRecordReader \n",
    "    _, value = reader.read(filename_queue)\n",
    "    feature = tf.decode_some(value)\n",
    "    # 对样例进行预处理\n",
    "    processed_example = some_processing(features)\n",
    "    return processed_example\n",
    "\n",
    "def input_pipeline(filename, batch_size, num_epoches=None):\n",
    "    # 当 num_epochs == None时表示文件队列总是可用的，一直循环入队\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, nochs=num_epochs, shuffle=True)  # 创建文件名队列，并shuffle\n",
    "    example = getum_ep_my_example(filename_queue)   # 通过 Reader 和 Decoder 将文件转换为特征张量\n",
    "    # min_after_dequeue 表示从样例队列中出队的样例个数,值越大表示打乱的越好\n",
    "    min_after_dequeue = 1000\n",
    "    # capacity 表示批数据队列的容量，推荐设置\n",
    "    # capacity = min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    # 创建批样例 example_batch\n",
    "    example_batch = tf.train.shuffle_batch(\n",
    "        [example], batch_size=batch_size, capacity=capacity,\n",
    "        min_after_dequeue = min_after_dequeue)\n",
    "    return example_batch   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 填充数据节点的方法\n",
    "不需要存储完整数据集，有效减小了内存开销．使用流水线也保证了实时性．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用上面定义的 input_pipeline 方法获取批样例 x_batch\n",
    "x_batch = input_pipeline(['stat.tfrecord'], batch_size=64)\n",
    "# 模型\n",
    "sess = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                  tf.local_variables_initializer())   # 在使用协调器管理多线程之前，先对其初始化\n",
    "sess.run(init_op)\n",
    "# 创建协调器\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "try:\n",
    "    for _ in range(1000):\n",
    "        if not coord.should_stop():\n",
    "            sess.run(train_op)\n",
    "            print(example)\n",
    "except tf.error.OutOfRangeError:\n",
    "    print(\"Catch OutOfRangeError\")\n",
    "finally:\n",
    "    #　请求停止后台线程\n",
    "    coord.request_stop()\n",
    "    print(\"Finish reading\")\n",
    "#　等待所有后台线程安全退出\n",
    "coord.join(threads)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动手实践\n",
    "以数据集 cifar-10 为例，采用 pipeline 输入。\n",
    "[cifar-input.py]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
