{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CS 20: TensorFlow for Deep Learning Research:](https://docs.google.com/presentation/d/1e1gE2JJXipWm1UJgor_y8pHcM8L8oMaCVtvQvZUBlQY/edit#slide=id.g2f115d1cc0_0_421)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager Execution\n",
    "- Motivation:  \n",
    "    - TensorFlow today: Construct a graph and execute it.    \n",
    "    This is declarative programming. Its benefits include performance and easy translation to other platforms; drawbacks include that declarative programming is non-Pythonic and difficult to debug.  \n",
    "    - What if you could execute operations directly?   \n",
    "    Eager execution offers just that: it is an imperative front-end to TensorFlow.    \n",
    "- Key advantages: Eager execution …    \n",
    "    - is compatible with Python debugging tools  \n",
    "        - pdb.set_trace() to your heart's content!  \n",
    "    - provides immediate error reporting  \n",
    "    - permits use of Python data structures  \n",
    "    - enables you to use and differentiate through Python control flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置和基本用法\n",
    "要启动 Eager Execution，请将 ```tf.enable_eager_execution()``` 添加到程序或控制台会话的开头。不要将此操作添加到程序调用的其他模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0-dev20180817\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tfe.enable_eager_execution() # Call this at program start-up\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在可以运行 Tensorflow 操作了，结果将立即返回："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfe.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = [[2.]] # No need for placeholders!\n",
    "m = tf.matmul(x,x)\n",
    "print(m)   # No sessions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "启用 Eager Execution 会改变 TensorFlow 操作的行为方式 - 现在它们会立即评估并将值返回给 Python。tf.Tensor 对象会引用具体值，而不是指向计算图中的节点的符号句柄。由于不需要构建稍后在会话中运行的计算图，因此使用 print() 或调试程序很容易检查结果。评估、输出和检查张量值不会中断计算梯度的流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Today: Declarative (Graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**优点 Graphs are:**  \n",
    "Optimizable  \n",
    "- automati nuffer reuse 自动缓存重用  \n",
    "- constant folding 不断折叠  \n",
    "- inter-op parallelism  并行操作  \n",
    "- automatic trade-off between compute and memory 计算与内存之间的自动权衡  \n",
    "\n",
    "Deployable  \n",
    "- the graph is an intermediate representation for models 图是模型的中间表示  \n",
    "\n",
    "Rewritable  \n",
    "- experiment with automatic device placement or quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**缺点 But graphs are also ...**  \n",
    "Difficult to debug  \n",
    "- errors are reported long after graph construction  \n",
    "- execution cannot be debugged with pdb or print statements  \n",
    "\n",
    "Un-Pythonic  \n",
    "- writing a TensorFlow program is an exercise in metaprogramming  \n",
    "- control flow (e.g., tf.while_loop) differs from Python  \n",
    "- can't easily mix graph construction with custom data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow 缺点：难以 debug，跟 numpy 之间无法通用，之前就遇到过 tf.bool ，必须用 tf.cond()  \n",
    "现在有了 Exger execution, 在也不用担心：  \n",
    "- placeholders  \n",
    "- sessions  \n",
    "- control dependencies  \n",
    "- \"lazy loading\"  \n",
    "- {name, variable, op} scopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Lazy Loading\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random_uniform([2,2])\n",
    "with tf.Session() as sess:\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            print(sess.run(x[i,j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the code here is what one might quickly hack up in the middle of their program to analyze the Tensor x.  \n",
    "It’s easy to miss, but each iteration of the loop is adding operations to the in-memory representation of the graph  \n",
    "每一次迭代都执行一次图的表示，占用了内存??? 是吗，可能因为这里是随机的？在神经网络训练的时候，变量都是可以保存的，并不会重新保存图吧。如果这里 x 用 Variable 包裹下，就不会有这样的情况了对吧？\n",
    "\n",
    "In this particular case, there is also the fact that each call to session.run is executing the random_uniform operation, so this snippet here isn’t printing a consistent snapshot of the tensor.  \n",
    "而是每迭代执行一次 session.run 都会重新执行 random_unifor 操作。因此每一次打印其实是不同的 x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新启动 jupyter kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tfe.enable_eager_execution() # Call this at program start-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random_uniform([2,2])\n",
    "for i in range(x.shape[0]):\n",
    "    for j in range(x.shape[1]):\n",
    "        print(x[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors Act Like NumPy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以用 numpy 的函数来处理 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = tf.constant([1.0,2.0,3.0])\n",
    "\n",
    "assert type(x.numpy()) == np.ndarray\n",
    "squared = np.square(x)  # Tensors are compatible with NumPy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in squared:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradients\n",
    "\n",
    "Automatic differentiation is built into eager execution\n",
    "\n",
    "Under the hood ...  \n",
    "- Operations are recorded on a tape  \n",
    "- The tape is played back to compute gradients  \n",
    "    - This is reverse-mode differentiation (backpropagation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "grad = tfe.gradients_function(square) # Differentiate w.r.t. input of square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(square(3.))\n",
    "print(grad(3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tfe.Variable(2.0)\n",
    "def loss(y):\n",
    "    return (y-x**2)**2\n",
    "\n",
    "grad = tfe.implicit_gradients(loss)\n",
    "print(loss(7.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad(7.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 都不用初始化了吗？\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APIs for computing gradients work even when eager execution is not enabled  \n",
    "- tfe.gradients_function()  \n",
    "- tfe.value_and_gradients_function()  \n",
    "- tfe.implicit_gradients()  \n",
    "- tfe.implicit_value_and_gradients()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A collection of operations\n",
    "TensorFlow = Operation Kernels + Execution  \n",
    "- Graph construction: Execute compositions of operations with Sessions  \n",
    "- Eager execution: Execute compositions with Python \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大部分 TF 的 API 是不管 eager execution 是否开启都可以使用的。  \n",
    "但是一旦 eager execution 被 enabled 后：  \n",
    "- prefer **tfe.Variable** under eager execution (compatible with graph construction)  \n",
    "- manage your own variable storage — variable collections are not supported!  没有变量管理了，需要自行管理变量  \n",
    "- use **tf.contrib.summary**  \n",
    "- use **tfe.Iterator** to iterate over datasets under eager execution  \n",
    "- prefer object-oriented layers (e.g., tf.layers.Dense) \n",
    "    - functional layers (e.g., tf.layers.dense) only work if wrapped in **tfe.make_template**  \n",
    "- prefer **tfe.py_func** over tf.py_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example 可参考 linear_eager.py\n",
    "其中一些平时不怎么用的 api 这里研究下，很奇怪在 pycharm 关于 tfe 的都不能直接用 ctrl+B 来看源代码，可是代码跑起来又没有问题。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import   # 绝对路径的引入\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "print(tfe.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True w: [[-2.0], [4.0], [1.0]]\n",
      "True b: [0.5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_w = [[-2.0],[4.0],[1.0]]  # list\n",
    "true_b = [0.5]\n",
    "noise_level = 0.01\n",
    "\n",
    "# Training constants\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(\"True w: %s\" % true_w)\n",
    "print(\"True b: %s\\n\" % true_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list 可以直接当做 tensor 用了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=6, shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(true_w)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> tf.Tensor([2 1], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [[1],[2]]\n",
    "b = [[1,2,3]]\n",
    "tf.matmul(a, b)\n",
    "print(type(a), tf.shape(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_dataset(w,b, noise_level, batch_size, num_batches):\n",
    "    \"\"\"tf.data.Dataset that yields synthetic data for linear regression.\"\"\"\n",
    "    return synthetic_dataset_helper(w, b,\n",
    "                                    tf.shape(w)[0], noise_level, batch_size,\n",
    "                                    num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_dataset_helper(w, b, num_features, noise_level, batch_size,\n",
    "                                num_batches):\n",
    "    \"\"\"\n",
    "\n",
    "    # w is a matrix with shape [N, M]\n",
    "    # b is a vector with shape [M]\n",
    "    # So:\n",
    "    # - Generate x's as vectors with shape [batch_size N]\n",
    "    # - y = tf.matmul(x, W) + b + noise\n",
    "    \"\"\"\n",
    "    def batch(_):\n",
    "        x = tf.random_normal([batch_size, num_features])  # [64, 3]\n",
    "        y = tf.matmul(x, w) + b + noise_level * tf.random_normal([]) # [64, 1]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    with tf.device(\"/device:GPU:0\"):\n",
    "        return tf.data.Dataset.range(num_batches).map(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tf.data.dataset``` 需要好好研究下，有了这个貌似就不需要 placeholder\n",
    "\n",
    "#### 使用 ```tf.Iterator()``` 迭代得到数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = synthetic_dataset(true_w, true_b, noise_level, batch_size, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((64, ?), (64, 1)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.eager.python.datasets.Iterator at 0x7f2a237e4cf8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfe.Iterator(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = tfe.Iterator(dataset).next()\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(tf.keras.Model):\n",
    "    \"\"\"A tensorflow linear regression model\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self._hidden_layer = tf.layers.Dense(1)\n",
    "\n",
    "    def call(self, xs, ys):\n",
    "        \"\"\"Invoke the linear model\n",
    "\n",
    "        :param xs: input features, as a tensor of size [batch_size, ndims].\n",
    "        :return:  the predictions of the linear mode, as a tensor of size [batch_size]\n",
    "        \"\"\"\n",
    "        logits = self._hidden_layer(xs)\n",
    "        ### 损失函数是均方差\n",
    "        return tf.reduce_mean(tf.square(tf.subtract(logits, ys)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keras 可以这么写。。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.layers.Dense(1)(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算梯度和loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.eager.backprop.implicit_val_and_grad.<locals>.grad_fn(*args, **kwds)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = lambda xs, ys: model(xs, ys)\n",
    "loss_and_grads = tfe.implicit_value_and_gradients(mse)\n",
    "loss_and_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loss_and_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: gpu:0\n"
     ]
    }
   ],
   "source": [
    "model = LinearModel()\n",
    "device = \"gpu:0\" if tfe.num_gpus() else \"cpu:0\"\n",
    "print(\"Using device: %s\" % device)\n",
    "with tf.device(device):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 执行 mini-batch 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 43.0392951965332\n",
      "Iteration 1: loss = 16.165969848632812\n",
      "Iteration 2: loss = 10.76777172088623\n",
      "Iteration 3: loss = 7.971755504608154\n",
      "Iteration 4: loss = 3.6797871589660645\n",
      "Iteration 5: loss = 2.895902156829834\n",
      "Iteration 6: loss = 1.5528433322906494\n",
      "Iteration 7: loss = 0.6692441701889038\n",
      "Iteration 8: loss = 0.5839046239852905\n",
      "Iteration 9: loss = 0.24569039046764374\n",
      "Iteration 10: loss = 0.2409191131591797\n",
      "Iteration 11: loss = 0.16683286428451538\n",
      "Iteration 12: loss = 0.11612279713153839\n",
      "Iteration 13: loss = 0.0448516346514225\n",
      "Iteration 14: loss = 0.04058341309428215\n",
      "Iteration 15: loss = 0.022977981716394424\n",
      "Iteration 16: loss = 0.02016042359173298\n",
      "Iteration 17: loss = 0.011417818255722523\n",
      "Iteration 18: loss = 0.007630700711160898\n",
      "Iteration 19: loss = 0.003977485932409763\n",
      "\n",
      "After training: w=[[-1.993047  ]\n",
      " [ 3.9545689 ]\n",
      " [ 0.97655946]]\n",
      "\n",
      "After training: b=[0.5055817]\n"
     ]
    }
   ],
   "source": [
    "for i,(xs, ys) in enumerate(tfe.Iterator(dataset)):\n",
    "        loss, grads = loss_and_grads(xs, ys)\n",
    "        print(\"Iteration {}: loss = {}\".format(i, loss.numpy()))\n",
    "        optimizer.apply_gradients(grads)\n",
    "\n",
    "print(\"\\nAfter training: w=%s\" % model.variables[0].numpy())\n",
    "print(\"\\nAfter training: b=%s\" % model.variables[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
