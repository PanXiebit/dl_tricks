{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tf.train.Saver``` 是辅助训练的工具，它实现了存储模型与checkpoint文件间的读写操作。 checkpoint 文件是以 <变量名，张量值> 的形式来序列化存储模型参数的二进制文件，它是用户持久化存储模型参数的推荐文件格式，拓展名为 ckpt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panxie/anaconda3/envs/NLP/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/panxie/anaconda3/envs/NLP/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "w = tf.Variable(tf.random_normal([1,4], stddev=0.01), name=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=tf.get_default_graph()) as sess:\n",
    "    writer = tf.summary.FileWriter(\"./graph\", graph=sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![01.png](attachment:01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.Variable向数据流图中添加了存储节点。存储节点是可以展开的子图，它包含3个操作和1个初始值。初始化操作```tf.global_variables_initializer```， 然后调用变量 w 的 Assign 操作， Assign 操作又依赖于 random_normal 操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在初始化的时候可以选择性的初始化部分变量，```tf.global_variables_initializer(var_list)``` var_list 是同类变量的集合，可以通过 collection 显示的指定。  \n",
    "\n",
    "tensorflow 中有5类内置的变量集合。\n",
    "\n",
    "|方法名称|类别关键字|类别说明|\n",
    "|---|---|---|\n",
    "|tf.global_variables|GraphKeys.GLOBAL_VARIABLES|跨设备的全局变量集合|\n",
    "|tf.local_vriables|GraphKeys.LOCAL_VARIABLES|进程内本地变量集合|\n",
    "|tf.model_variables|GraphKeys.MODEL_VARIABLES|进程内存储模型参数的变量集合|\n",
    "|tf.trainable_variables|GraphKeys.TRAINABLE_VARIABLES|存储需要训练的模型参数的变量集合|\n",
    "|tf.moving_average_variables|GraphKeys.MOVING_AVERAGE_VARIABLES|使用指数移动平均的变量集合|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Variable 'w:0' shape=(1, 4) dtype=float32_ref>],\n",
       " [],\n",
       " [],\n",
       " [<tf.Variable 'w:0' shape=(1, 4) dtype=float32_ref>],\n",
       " [])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.global_variables(), tf.local_variables(), tf.model_variables(), tf.trainable_variables(),tf.moving_average_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更新模型参数\n",
    "在数据流图不变的情况下，存储节点状态的不同得到不同的输出。  \n",
    "\n",
    "|方法名称|功能说明|\n",
    "|---|---|\n",
    "|tf.assign|直接赋值|\n",
    "|tf.assign_add|加法赋值|\n",
    "|tf.assign_sub|减法赋值|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 Tensorflow 进行训练时，优化器的 apply_gradients 成员方法内部也会调用上上个表格中的方法进行模型参数的更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型保存和恢复"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当你训练好一个神经网络后，你会想保存好你的模型便于以后使用并且用于生产。因此，什么是Tensorflow模型？Tensorflow模型主要包含网络设计（或者网络图）和训练好的网络参数的值。所以Tensorflow模型有两个主要的文件："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Meta图:  \n",
    "Meta图是一个协议缓冲区（protocol buffer），它保存了完整的Tensorflow图；比如所有的变量、运算、集合等。这个文件的扩展名是.meta.  \n",
    "\n",
    "b) Checkpoint 文件  \n",
    "这是一个二进制文件，它保存了权重、偏置项、梯度以及其他所有的变量的取值，扩展名为.ckpt。但是， 从0.11版本开始，Tensorflow对改文件做了点修改，checkpoint文件不再是单个.ckpt文件，而是如下两个文件：\n",
    "\n",
    "mymodel.data-00000-of-00001  \n",
    "mymodel.index\n",
    "\n",
    "其中， .data文件包含了我们的训练变量。除此之外，还有一个叫checkpoint的文件，它保留了最新的checkpoint文件的记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7fd174087c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/panxie/anaconda3/envs/NLP/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 710, in __del__\n",
      "    if self._session is not None:\n",
      "AttributeError: 'Session' object has no attribute '_session'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./my_test_model'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "w1 = tf.Variable(tf.random_normal(shape=[2]), name='w1')\n",
    "w2 = tf.Variable(tf.random_normal(shape=[5]), name='w2')\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.save(sess, './my_test_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们要在1000次迭代后保存模型，我们应该在调用保存方法时传入步数计数：\n",
    "```python\n",
    "saver.save(sess, \"my_test_model\", global_step=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你只想保留4个最新的模型并且在训练过程中每过2小时保存一次模型，你可以使用max_to_keep和keep_checkpoint_every_n_hours，就像这样：\n",
    "```python\n",
    "#saves a model every 2 hours and maximum 4 latest models are saved.\n",
    "saver = tf.train.Saver(max_to_keep=4, keep_checkpoint_every_n_hours=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，如果我们在tf.train.Saver()中不指定任何东西，它将保存所有的变量。要是我们不想保存所有的变量而只是一部分变量。我们可以指定我们想要保存的变量/集合。当创建tf.train.Saver()对象的时候，我们给它传递一个我们想要保存的变量的字典列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_model/test_save-1000'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal(shape=[2]), name='w1')\n",
    "w2 = tf.Variable(tf.random_normal(shape=[5]), name='w2')\n",
    "saver = tf.train.Saver([w1,w2])\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.save(sess, './my_model/test_save',global_step=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入预训练模型\n",
    "如果你想要用其他人预训练的模型进行微调，需要做两件事：  \n",
    "\n",
    "a) 创建网络   \n",
    "你可以写python代码来手动创建和原来一样的模型。但是，想想看，我们已经将原始网络保存在了.meta文件中，可以用tf.train.import()函数来重建网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"./my_model/test_save-1000.meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model/test_save-1000\n",
      "[<tf.Variable 'w1:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'w2:0' shape=(5,) dtype=float32_ref>, <tf.Variable 'w1:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'w2:0' shape=(5,) dtype=float32_ref>, <tf.Variable 'w1:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'w2:0' shape=(5,) dtype=float32_ref>]\n",
      "[ 0.5913126  -0.11641493]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph(\"./my_model/test_save-1000.meta\")\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint(\"./my_model/\"))\n",
    "    print(tf.global_variables())\n",
    "    print(sess.run(\"w1:0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用恢复的模型\n",
    "现在你已经理解如何保存和恢复Tensorflow模型，我们来写一个实际的示例来恢复任何预训练的模型并用它来预测、微调或者进一步训练。无论你什么时候用Tensorflow，你都会定义一个网络，它有一些样本（训练数据）和超参数（如学习率、迭代次数等）。通常用一个占位符（placeholder）来将所有的训练数据和超参数输入给网络。下面我们用占位符建立一个小型网络并保存它。注意，当网络被保存的时候，占位符中的值并没有被保存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "w1 = tf.placeholder(dtype=tf.float32, name=\"w1\")\n",
    "w2 = tf.placeholder(dtype=tf.float32, name=\"w2\")\n",
    "b1 = tf.Variable(2.0, name='bias')\n",
    "feed_dict = {w1:4, w2:8}\n",
    "\n",
    "#Define a test operation that we will restore\n",
    "w3 = tf.add(w1,w2)\n",
    "w4 = tf.multiply(w3, b1, name=\"op-to-restore\")\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(w4, feed_dict=feed_dict))\n",
    "    saver.save(sess, \"./my_second_model/test_save\", global_step=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们想要恢复这个网络的时候，我们不仅需要恢复图和权重，还需要准备一个新的feed_dict来将训练数据输入到网络中。我们可以通过graph.get_tensor_by_name方法来引用这些保存的运算和占位符变量。\n",
    "```python\n",
    "w1 = graph.get_tensor_by_name(\"w1:0\")\n",
    "op_to_restore = graph.get_tensor_by_name(\"op-to-restore:0\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'bias:0' shape=() dtype=float32_ref>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们只是想用不同的数据运行相同的网络，你可以方便地用feed_dict将新的数据送到网络中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_second_model/test_save-1000\n",
      "60.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "saver = tf.train.import_meta_graph(\"./my_second_model/test_save-1000.meta\")\n",
    "saver.restore(sess, tf.train.latest_checkpoint(\"./my_second_model/\"))\n",
    "\n",
    "#How to access saved variable/Tensor/placeholders \n",
    "graph = tf.get_default_graph()\n",
    "w1 = graph.get_tensor_by_name(\"w1:0\")\n",
    "w2 = graph.get_tensor_by_name(\"w2:0\")\n",
    "feed_dict = {w1:13.0, w2:17.0}\n",
    "## How to access saved operation\n",
    "op_to_restore = graph.get_tensor_by_name(\"op-to-restore:0\")\n",
    "\n",
    "print(sess.run(op_to_restore, feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 迁移学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要是你想在原来的计算图中通过添加更多的层来增加更多的运算并且训练。当然也可以实现，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_second_model/test_save-1000\n",
      "120.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess=tf.Session()    \n",
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('./my_second_model/test_save-1000.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./my_second_model/'))\n",
    "\n",
    "#Now, let's access and create placeholders variables and\n",
    "# create feed-dict to feed new data\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "w1 = graph.get_tensor_by_name(\"w1:0\")\n",
    "w2 = graph.get_tensor_by_name(\"w2:0\")\n",
    "feed_dict ={w1:13.0,w2:17.0}\n",
    "\n",
    "#Now, access the op that you want to run. \n",
    "op_to_restore = graph.get_tensor_by_name(\"op-to-restore:0\")\n",
    "\n",
    "#Add more to the current graph\n",
    "add_on_op = tf.multiply(op_to_restore,2)\n",
    "\n",
    "print(sess.run(add_on_op,feed_dict))\n",
    "#This will print 120."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，我们能够只恢复原来图中的一部分然后添加一些其它层来微调吗？当然可以，只要通过graph.get_tensor_by_name()方法来获取原网络的部分计算图并在上面继续建立新计算图。这里给出了一个实际的例子。我们用meta图导入了一个预训练的vgg网络，然后将最后一层的输出个数改成2用于微调新的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph('vgg.meta')\n",
    "# Access the graph\n",
    "graph = tf.get_default_graph()\n",
    "## Prepare the feed_dict for feeding data for fine-tuning \n",
    "\n",
    "#Access the appropriate output for fine-tuning\n",
    "fc7= graph.get_tensor_by_name('fc7:0')\n",
    "\n",
    "#use this if you only want to change gradients of the last layer\n",
    "fc7 = tf.stop_gradient(fc7) # It's an identity function\n",
    "fc7_shape= fc7.get_shape().as_list()\n",
    "\n",
    "num_outputs=2\n",
    "weights = tf.Variable(tf.truncated_normal([fc7_shape[3], num_outputs], stddev=0.05))\n",
    "biases = tf.Variable(tf.constant(0.05, shape=[num_outputs]))\n",
    "output = tf.matmul(fc7, weights) + biases\n",
    "pred = tf.nn.softmax(output)\n",
    "\n",
    "# Now, you run this with fine-tuning data in sess.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 变量作用域"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tf.Variable```和 ```tf.get_variable```的区别:   \n",
    "- 前者在重复调用时会新建一个变量（如果变量值发生了变化，再次调用时值是否会变化呢？在滑动平均里面有用到），这样会占用更多的内存  \n",
    "- 后者可以通过设置 reuse 来调用同一个变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 首先验证重复调用函数会新建变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'w1_1:0' shape=() dtype=int32_ref>\n",
      "<tf.Variable 'w1_2:0' shape=() dtype=int32_ref>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo():\n",
    "    w1 = tf.Variable(0, name=\"w1\")\n",
    "    print(w1)\n",
    "foo(), foo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 然后验证心中的疑惑\n",
    "新建的变量是否会保存变量的值，在神经网络训练时，比如重复调用 batch_norm 函数，并没有新建变量，所以值还是会保存的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'w1_3:0' shape=() dtype=int32_ref>\n",
      "2\n",
      "<tf.Variable 'w1_3:0' shape=() dtype=int32_ref>\n",
      "4\n",
      "<tf.Variable 'w1_3:0' shape=() dtype=int32_ref>\n",
      "6\n",
      "<tf.Variable 'w1_3:0' shape=() dtype=int32_ref>\n",
      "8\n",
      "<tf.Variable 'w1_3:0' shape=() dtype=int32_ref>\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def foo():\n",
    "    w1 = tf.Variable(initial_value=0, name=\"w1\")\n",
    "    w2 = tf.assign(w1, tf.add(w1, tf.constant(2)))\n",
    "    return w1, w2\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    w1, w2 = foo()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(5):\n",
    "        print(w1)    # 这里也并没有重复调用 foo 函数，graph 已经确定了，只是在调用里面的 w1 值和计算方式\n",
    "        print(w2.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.get_variable 中 reuse 的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes!\n"
     ]
    }
   ],
   "source": [
    "def foo():\n",
    "    with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "        return v\n",
    "\n",
    "v1 = foo()  # Creates v.\n",
    "v2 = foo()  # Gets the same, existing v.\n",
    "assert(v1 == v2)\n",
    "print(\"yes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"fooo\"):  # reuse默认为None，\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "with tf.variable_scope(\"fooo\", reuse=True): # 如果第一次使用时设置为 True 会报错\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "assert v1 == v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "  def get_variable(self,\n",
    "                   name,\n",
    "                   shape=None,\n",
    "                   dtype=dtypes.float32,\n",
    "                   initializer=None,\n",
    "                   regularizer=None,\n",
    "                   reuse=None,\n",
    "                   trainable=None,\n",
    "                   collections=None,\n",
    "                   caching_device=None,\n",
    "                   partitioner=None,\n",
    "                   validate_shape=True,\n",
    "                   use_resource=None,\n",
    "                   custom_getter=None,\n",
    "                   constraint=None,\n",
    "                   synchronization=VariableSynchronization.AUTO,\n",
    "                   aggregation=VariableAggregation.NONE):\n",
    "    \"\"\"\n",
    "    - Set `reuse` to `True` when you only want to reuse existing Variables.\n",
    "    - Set `reuse` to `False` when you only want to create new Variables.\n",
    "    - Set `reuse` to None (the default) or tf.AUTO_REUSE when you want\n",
    "    \n",
    "    - regularizer: A (Tensor -> Tensor or None) function; the result of\n",
    "        applying it on a newly created variable will be added to the collection\n",
    "        GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.\n",
    "    - validate_shape: If False, allows the variable to be initialized with a\n",
    "        value of unknown shape. If True, the default, the shape of initial_value\n",
    "        must be known. 这个参数很有用啊，当你在设置一个新的变量的使用，可能要与 placeholder 保持一致，\n",
    "        会有某个维度是 None，之前就遇到过，然后用 tf.Variable代替的，现在可以这么用了？\n",
    "        试了下并没有卵用。。。\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还是会报错\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "aa = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "\n",
    "bb = tf.get_variable(\"bb\", shape=aa.get_shape(), initializer=tf.constant_initializer(1),validate_shape=False)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(bb.eval(), feed_dict={aa:tf.constant(5)})  # Shape of a new variable (bb) must be fully defined, but instead was (?, 2, 3)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命令行参数\n",
    "参数包括两种：模型超参数和集群参数。解析命令行参数主要有两种方案： argparse 和 tf.app.flags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 argparse 解析命令行参数\n",
    "##### 创建解析器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(prog='demo', description='A demo program', epilog=\"The end of the usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: demo [-h]\n",
      "\n",
      "A demo program\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help  show this help message and exit\n",
      "\n",
      "The end of the usage\n"
     ]
    }
   ],
   "source": [
    "parser.print_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里没有显示设置 usage 参数，默认根据用户添加的参数自动生成使用方法。但是这里我们没有添加参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 添加待解析参数\n",
    "\n",
    "|参数名称|功能说明|\n",
    "|---|---|\n",
    "|name or flags|名称或标记|\n",
    "|action|解析参数成功时触发的动作|\n",
    "|nargs|待解析参数的个数|\n",
    "|const|action和nargs参数可能使用的常量值|\n",
    "|default|待解析参数的默认值|\n",
    "|type|解析参数后保存的类型|\n",
    "|choices|参数可选值集合，用于约束枚举型参数|\n",
    "|required|是否必须从参数中解析的参数，默认为True|\n",
    "|help|参数功能说明|\n",
    "|dest|解析参数后保存的对象名称|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(prog=\"demo\", description=\"A demo program\", epilog='The end od usage')\n",
    "\n",
    "parser.add_argument(\"name\")\n",
    "parser.add_argument(\"-a\", '--age',action=\"store\", type=int, required=True)\n",
    "parser.add_argument('-s','--status',choices=['alpha', 'beta', 'released'], type=str, dest='mystatus')\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 tf.app.flags 解析命令行参数\n",
    "tf.app.flags 简化了 argparse 中解析器的大量配置选项，仅实现参数解析、默认值和打印帮助信息等基本功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flags 模块定义参数的方法：  \n",
    "\n",
    "|方法名称|调用样例|\n",
    "|---|---|\n",
    "|DEFINE_float|flags.DEFINE_float(\"learning_rate\",0.01, \"learning rate\")|\n",
    "|DEFINE_integer|flags.DEFINE_integer(\"batch_szie\",100, \"Training batch size\")|\n",
    "|DEFINE_string|flags.DEFINE_string(\"data_dir\",'/mnist/data', \"directory for storing data\")|\n",
    "|DEFINE_boolean|flags.DEFINE_boolean(\"inference_only\",False, \"only perform inferencing\")|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在就去找个以前的代码改进下～"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
