{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文文本挖掘预处理特点\n",
    "\n",
    "参考：https://www.cnblogs.com/pinard/p/6744056.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们看看中文文本挖掘预处理和英文文本挖掘预处理相比的一些特殊点。  \n",
    "\n",
    "首先，中文文本是没有像英文的单词空格那样隔开的，因此不能直接像英文一样可以直接用最简单的空格和标点符号完成分词。所以一般我们需要用分词算法来完成分词，在[文本挖掘的分词原理](http://www.cnblogs.com/pinard/p/6677078.html)中，我们已经讲到了中文的分词原理，这里就不多说。\n",
    "\n",
    "第二，中文的编码不是utf8，而是unicode。这样会导致在分词的时候，和英文相比，我们要处理编码的问题。\n",
    "\n",
    "这两点构成了中文分词相比英文分词的一些不同点，后面我们也会重点讲述这部分的处理。当然，英文分词也有自己的烦恼，这个我们在以后再讲。了解了中文预处理的一些特点后，我们就言归正传，通过实践总结下中文文本挖掘预处理流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集收集\n",
    "\n",
    "在文本挖掘之前，我们需要得到文本数据，文本数据的获取方法一般有两种：使用别人做好的语料库和自己用爬虫去在网上去爬自己的语料数据。\n",
    "\n",
    "对于第一种方法，常用的文本语料库在网上有很多，如果大家只是学习，则可以直接下载下来使用，但如果是某些特殊主题的语料库，比如“机器学习”相关的语料库，则这种方法行不通，需要我们自己用第二种方法去获取。\n",
    "\n",
    "对于第二种使用爬虫的方法，开源工具有很多，通用的爬虫我一般使用[beautifulsoup](https://www.crummy.com/software/BeautifulSoup/)。但是我们我们需要某些特殊的语料数据，比如上面提到的“机器学习”相关的语料库，则需要用主题爬虫（也叫聚焦爬虫）来完成。这个我一般使用[ache](https://github.com/ViDA-NYU/ache)。 ache允许我们用关键字或者一个分类算法来过滤出我们需要的主题语料，比较强大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 除去数据中非文本部分\n",
    "\n",
    "这一步主要是针对我们用爬虫收集的语料数据，由于爬下来的内容中有很多html的一些标签，需要去掉。少量的非文本内容的可以直接用Python的正则表达式(re)删除, 复杂的则可以用beautifulsoup来去除。去除掉这些非文本的内容后，我们就可以进行真正的文本预处理了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理中文编码问题\n",
    "由于Python2不支持unicode的处理，因此我们使用Python2做中文文本预处理时需要遵循的原则是，存储数据都用utf8，读出来进行中文相关处理时，使用GBK之类的中文编码，在下面一节的分词时，我们再用例子说明这个问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文分词\n",
    "常用的中文分词软件有很多，个人比较推荐结巴分词。安装也很简单，比如基于Python的，用\"pip install jieba\"就可以完成。下面我们就用例子来看看如何中文分词。\n",
    "\n",
    "首先我们准备了两段文本，这两段文本在两个文件中。两段文本的内容分别是nlp_test0.txt和nlp_test2.txt："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Tokenizer.cut at 0x7f6a84cf09e8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "with open(\"./nlp_test1.txt\") as f:\n",
    "    document = f.read() # 如果是python2，则需要用 decode(\"GBK\")\n",
    "    document_cut = jieba.cut(document)\n",
    "document_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.438 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'        沙 瑞金 赞叹 易 学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易 学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易 学习 被 降职 到 道口 县当 县长 ， 王 大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王 大路 ， 就 和 易 学习 一起 给 王 大路 凑 了 5 万块 钱 ， 王 大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王 大路 竟然 做 得 风生水 起 。 沙 瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。 \\n \\n         沙 瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王 大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王 大路 就 在 京州帝 豪园 买 了 三套 别墅 ， 可是 李达 康和易 学习 都 不要 ， 这些 房子 都 在 王 大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = \" \".join(document_cut)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./nlp_test2.txt\", \"w\") as f2:\n",
    "    f2.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现对于一些人名和地名，jieba处理的不好，不过我们可以帮jieba加入词汇如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.suggest_freq('沙瑞金', True)\n",
    "jieba.suggest_freq('易学习', True)\n",
    "jieba.suggest_freq('王大路', True)\n",
    "jieba.suggest_freq('京州', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以在很多 NLP 任务中先做命令实体识别的意义就在这里对吧?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        沙瑞金 赞叹 易学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易学习 被 降职 到 道口 县当 县长 ， 王大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王大路 ， 就 和 易学习 一起 给 王大路 凑 了 5 万块 钱 ， 王大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王大路 竟然 做 得 风生水 起 。 沙瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。 \n",
      " \n",
      "         沙瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王大路 就 在 京州 帝豪园 买 了 三套 别墅 ， 可是 李达康 和 易学习 都 不要 ， 这些 房子 都 在 王大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。\n"
     ]
    }
   ],
   "source": [
    "with open(\"./nlp_test1.txt\", \"r\") as f1:\n",
    "    text = f1.read()\n",
    "    text_cut = jieba.cut(text)  # list\n",
    "    result = \" \".join(text_cut)\n",
    "    print(result)\n",
    "    with open(\"./nlp_test2.txt\", \"w\") as f2:\n",
    "        f2.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入停用词\n",
    "在上面我们解析的文本中有很多无效的词，比如“着”，“和”，还有一些标点符号，这些我们不想在文本分析的时候引入，因此需要去掉，这些词就是停用词。常用的中文停用词表是1208个，[下载地址在这](http://files.cnblogs.com/files/pinard/stop_words.zip)。当然也有其他版本的停用词表，不过这个1208词版是我常用的。\n",
    "\n",
    "在我们用scikit-learn做特征处理的时候，可以通过参数stop_words来引入一个数组作为停用词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpword_path = \"stop_words.txt\"\n",
    "with open(stpword_path, encoding=\"gbk\") as f:\n",
    "    stpword_content = f.read()\n",
    "    stpword_list = stpword_content.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '?', '、', '。', '“', '”', '《', '》', '！', '，', '：', '；', '？', '人民', '末##末', '啊', '阿', '哎', '哎呀', '哎哟', '唉', '俺', '俺们', '按', '按照', '吧', '吧哒', '把', '罢了', '被', '本', '本着', '比', '比方', '比如', '鄙人', '彼', '彼此', '边', '别', '别的', '别说', '并', '并且', '不比', '不成', '不单', '不但', '不独', '不管', '不光', '不过', '不仅', '不拘', '不论', '不怕', '不然', '不如', '不特', '不惟', '不问', '不只', '朝', '朝着', '趁', '趁着', '乘', '冲', '除', '除此之外', '除非', '除了', '此', '此间', '此外', '从', '从而', '打', '待', '但', '但是', '当', '当着', '到', '得', '的', '的话', '等', '等等', '地', '第', '叮咚', '对', '对于', '多', '多少', '而', '而况', '而且', '而是']\n"
     ]
    }
   ],
   "source": [
    "print(stpword_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征处理\n",
    "现在我们就可以用scikit-learn来对我们的文本特征进行处理了，在[文本挖掘预处理之向量化与Hash Trick中](http://www.cnblogs.com/pinard/p/6688348.html)，我们讲到了两种特征处理的方法，向量化与Hash Trick。而向量化是最常用的方法，因为它可以接着进行TF-IDF的特征处理。在[文本挖掘预处理之TF-IDF](http://www.cnblogs.com/pinard/p/6693230.html)中，我们也讲到了TF-IDF特征处理的方法。这里我们就用scikit-learn的TfidfVectorizer类来进行TF-IDF特征处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向量化与 Hash Trick\n",
    "##### 词袋模型\n",
    "在讲向量化与Hash Trick之前，我们先说说词袋模型(Bag of Words,简称BoW)。词袋模型假设我们不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重。而权重与词在文本中出现的频率有关。\n",
    "\n",
    "词袋模型首先会进行分词，在分词之后，通过统计每个词在文本中出现的次数，我们就可以得到该文本基于词的特征，如果将各个文本样本的这些词与对应的词频放在一起，就是我们常说的向量化。向量化完毕后一般也会使用TF-IDF进行特征的权重修正，再将特征进行标准化。 再进行一些其他的特征工程后，就可以将数据带入机器学习算法进行分类聚类了。\n",
    "\n",
    "总结下词袋模型的三部曲：分词（tokenizing），统计修订词特征值（counting）与标准化（normalizing）。\n",
    "\n",
    "词袋模型有很大的局限性，因为它仅仅考虑了词频，没有考虑上下文的关系，因此会丢失一部分文本的语义。但是大多数时候，如果我们的目的是分类聚类，则词袋模型表现的很好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 词袋模型之向量化\n",
    "在词袋模型的统计词频这一步，我们会得到该文本中所有词的词频，有了词频，我们就可以用词向量表示这个文本。这里我们举一个例子，例子直接用scikit-learn的CountVectorizer类来完成，这个类可以帮我们完成文本的词频统计与向量化，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 15)\t2\n",
      "  (0, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 3)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 7)\t1\n",
      "  (3, 10)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 11)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 17)\t1\n",
      "  (3, 13)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 15)\t1\n"
     ]
    }
   ],
   "source": [
    "corpus=[\"I come to China to travel\", \n",
    "    \"This is a car polupar in China\",          \n",
    "    \"I love tea and Apple \",   \n",
    "    \"The work is to write some papers in science\"] \n",
    "print(vectorizer.fit_transform(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出4个文本的词频已经统计出，在输出中，左边的括号中的第一个数字是文本的序号，第2个数字是词的序号，注意词的序号是基于所有的文档的。第三个数字就是我们的词频。\n",
    "\n",
    "我们可以进一步看看每个文本的词向量特征和各个特征代表的词，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]\n",
      " [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.fit_transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'apple', 'car', 'china', 'come', 'in', 'is', 'love', 'papers', 'polupar', 'science', 'some', 'tea', 'the', 'this', 'to', 'travel', 'work', 'write']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是先统计整个文本corpus, 去掉停用词，剩下的词就是向量的维度。然后统计每一行文字出现的词频，得到相应的向量。显然词表是按照字母顺序排序的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到我们一共有19个词，所以4个文本都是19维的特征向量。而每一维的向量依次对应了下面的19个词。另外由于词\"I\"在英文中是停用词，不参加词频的统计。\n",
    "\n",
    "由于大部分的文本都只会使用词汇表中的很少一部分的词，因此我们的词向量中会有大量的0。也就是说词向量是稀疏的。在实际应用中一般使用稀疏矩阵来存储。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **这里有个疑问？** 向量化之后的维度是根据自己的数据集来定，为什么不就是词表大小呢。这里是根据自己的数据集来的，但我们对测试集分类时，会出现 UNK 词吧，但是这个词其实在词表中是有的。那么在训练集中如果加上这个维度，其实也没有太大意义，因为在训练集中这个维度上所有的值都为0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将文本做了词频统计后，我们一般会通过TF-IDF进行词特征值修订，这部分我们后面再讲。\n",
    "\n",
    "向量化的方法很好用，也很直接，但是在有些场景下很难使用，比如分词后的词汇表非常大，达到100万+，此时如果我们直接使用向量化的方法，将对应的样本对应特征矩阵载入内存，有可能将内存撑爆，在这种情况下我们怎么办呢？第一反应是我们要进行特征的降维，说的没错！而Hash Trick就是非常常用的文本特征降维方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hash Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在大规模的文本处理中，由于特征的维度对应分词词汇表的大小，所以维度可能非常恐怖，此时需要进行降维，不能直接用我们上一节的向量化方法。而最常用的文本降维方法是Hash Trick。说到Hash，一点也不神秘，学过数据结构的同学都知道。这里的Hash意义也类似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Hash Trick里，我们会定义一个特征Hash后对应的哈希表的大小，这个哈希表的维度会远远小于我们的词汇表的特征维度，因此可以看成是降维。具体的方法是，对应任意一个特征名，我们会用Hash函数找到对应哈希表的位置，然后将该特征名对应的词频统计值累加到该哈希表位置。如果用数学语言表示,假如哈希函数h使第i个特征哈希到位置j,即 $h(i)=j$,则第i个原始特征的词频数值 $\\phi(i)$ 将累加到哈希后的第j个特征的词频数值 $\\hat \\phi(i)$上，即："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat \\phi(i)=\\sum_{i\\in J;h(i)=j}\\phi(i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 J 是原始特征的维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是上面的方法有一个问题，有可能两个原始特征的哈希后位置在一起导致词频累加特征值突然变大，为了解决这个问题，出现了hash Trick的变种signed hash trick,此时除了哈希函数h,我们多了一个一个哈希函数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\xi:N\\rightarrow \\pm1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时我们有\n",
    "$$\\hat \\phi(j)=\\sum_{i\\in J;h(i)=j}\\phi(i)\\xi(i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样做的好处是，哈希后的特征仍然是一个无偏的估计，不会导致某些哈希位置的值过大。\n",
    "\n",
    "当然，大家会有疑惑，这种方法来处理特征，哈希后的特征是否能够很好的代表哈希前的特征呢？从实际应用中说，由于文本特征的高稀疏性，这么做是可行的。如果大家对理论上为何这种方法有效，建议参考论文：[Feature hashing for large scale multitask learning](http://alex.smola.org/papers/2009/Weinbergeretal09.pdf).这里就不多说了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在scikit-learn的HashingVectorizer类中，实现了基于signed hash trick的算法，这里我们就用HashingVectorizer来实践一下Hash Trick，为了简单，我们使用上面的19维词汇表，并哈希降维到6维。当然在实际应用中，19维的数据根本不需要Hash Trick，这里只是做一个演示，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t2.0\n",
      "  (0, 2)\t-1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t-1.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (1, 2)\t-1.0\n",
      "  (1, 5)\t-1.0\n",
      "  (2, 0)\t2.0\n",
      "  (2, 5)\t-2.0\n",
      "  (3, 0)\t0.0\n",
      "  (3, 1)\t4.0\n",
      "  (3, 2)\t-1.0\n",
      "  (3, 3)\t1.0\n",
      "  (3, 5)\t-1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vectorizer2 = HashingVectorizer(n_features=6, norm=None)\n",
    "print(vectorizer2.fit_transform(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家可以看到结果里面有负数，这是因为我们的哈希函数ξ可以哈希到1或者-1导致的。\n",
    "\n",
    "和PCA类似，Hash Trick降维后的特征我们已经不知道它代表的特征名字和意义。此时我们不能像上一节向量化时候可以知道每一列的意义，所以Hash Trick的解释性不强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 向量化与 Hash Track 小结\n",
    "这里我们对向量化与它的特例Hash Trick做一个总结。在特征预处理的时候，我们什么时候用一般意义的向量化，什么时候用Hash Trick呢？标准也很简单。\n",
    "\n",
    "一般来说，只要词汇表的特征不至于太大，大到内存不够用，肯定是使用一般意义的向量化比较好。因为向量化的方法解释性很强，我们知道每一维特征对应哪一个词，进而我们还可以使用TF-IDF对各个词特征的权重修改，进一步完善特征的表示。\n",
    "\n",
    "而Hash Trick用大规模机器学习上，此时我们的词汇量极大，使用向量化方法内存不够用，而使用Hash Trick降维速度很快，降维后的特征仍然可以帮我们完成后续的分类和聚类工作。当然由于分布式计算框架的存在，其实一般我们不会出现内存不够的情况。因此，实际工作中我使用的都是特征向量化。\n",
    "\n",
    "向量化与Hash Trick就介绍到这里，下一篇我们讨论TF-IDF。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本向量化特征的不足\n",
    "在将文本分词并向量化后，我们可以得到词汇表中每个词在各个文本中形成的词向量，比如在文本挖掘预处理之向量化与Hash Trick这篇文章中，我们将下面4个短文本做了词频统计：\n",
    "```\n",
    "corpus=[\"I come to China to travel\", \n",
    "    \"This is a car polupar in China\",          \n",
    "    \"I love tea and Apple \",   \n",
    "    \"The work is to write some papers in science\"] \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不考虑停用词，处理后得到的词向量如下：  \n",
    "```\n",
    "[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]\n",
    " [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
    " [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
    " [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们直接将统计词频后的19维特征做为文本分类的输入，会发现有一些问题。比如第一个文本，我们发现\"come\",\"China\"和“Travel”各出现1次，而“to“出现了两次。似乎看起来这个文本与”to“这个特征更关系紧密。但是实际上”to“是一个非常普遍的词，几乎所有的文本都会用到，因此虽然它的词频为2，但是重要性却比词频为1的\"China\"和“Travel”要低的多。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF概述\n",
    "TF-IDF是Term Frequency -  Inverse Document Frequency的缩写，即“词频-逆文本频率”。它由两部分组成，TF和IDF。\n",
    "\n",
    "前面的TF也就是我们前面说到的词频，我们之前做的向量化也就是做了文本中各个词的出现频率统计，并作为文本特征，这个很好理解。关键是后面的这个IDF，即“逆文本频率”如何理解。在上一节中，我们讲到几乎所有文本都会出现的\"to\"其词频虽然高，但是重要性却应该比词频低的\"China\"和“Travel”要低。我们的IDF就是来帮助我们来反应这个词的重要性的，进而修正仅仅用词频表示的词特征值。\n",
    "\n",
    "概括来讲， IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如上文中的“to”。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专业的名词如“Machine Learning”。这样的词IDF值应该高。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面是从定性上说明的IDF的作用，那么如何对一个词的IDF进行定量分析呢？这里直接给出一个词x的IDF的基本公式如下：\n",
    "$$IDF(x)=\\dfrac{N}{N(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，N代表语料库中文本的总数，而 $N(x)$ 代表语料库中包含词x的文本总数。为什么IDF的基本公式应该是是上面这样的而不是像 $N/N(x)$ 这样的形式呢？这就涉及到信息论相关的一些知识了。感兴趣的朋友建议阅读吴军博士的《数学之美》第11章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的IDF公式已经可以使用了，但是在一些特殊的情况会有一些小问题，比如某一个生僻词在语料库中没有，这样我们的分母为0， IDF没有意义了。所以常用的IDF我们需要做一些平滑，使语料库中没有出现的词也可以得到一个合适的IDF值。平滑的方法有很多种，最常见的IDF平滑后的公式之一为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$IDF(x)=log\\dfrac{N+1}{N(x)+1}+1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了IDF的定义，我们就可以计算某一个词的TF-IDF值了：\n",
    "$$\\text{TF-IDF(x)}=TF(x)*IDF(x)$$\n",
    "其中TF(x)指词x在当前文本中的词频。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用scikit-learn进行TF-IDF预处理\n",
    "\n",
    "在scikit-learn中，有两种方法进行TF-IDF的预处理。\n",
    "\n",
    "第一种方法是在用CountVectorizer类向量化之后再调用TfidfTransformer类进行预处理。第二种方法是直接用TfidfVectorizer完成向量化与TF-IDF预处理。\n",
    "\n",
    "首先我们来看第一种方法，CountVectorizer+TfidfTransformer的组合，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t0.4424621378947393\n",
      "  (0, 15)\t0.697684463383976\n",
      "  (0, 3)\t0.348842231691988\n",
      "  (0, 16)\t0.4424621378947393\n",
      "  (1, 3)\t0.3574550433419527\n",
      "  (1, 14)\t0.45338639737285463\n",
      "  (1, 6)\t0.3574550433419527\n",
      "  (1, 2)\t0.45338639737285463\n",
      "  (1, 9)\t0.45338639737285463\n",
      "  (1, 5)\t0.3574550433419527\n",
      "  (2, 7)\t0.5\n",
      "  (2, 12)\t0.5\n",
      "  (2, 0)\t0.5\n",
      "  (2, 1)\t0.5\n",
      "  (3, 15)\t0.2811316284405006\n",
      "  (3, 6)\t0.2811316284405006\n",
      "  (3, 5)\t0.2811316284405006\n",
      "  (3, 13)\t0.3565798233381452\n",
      "  (3, 17)\t0.3565798233381452\n",
      "  (3, 18)\t0.3565798233381452\n",
      "  (3, 11)\t0.3565798233381452\n",
      "  (3, 8)\t0.3565798233381452\n",
      "  (3, 10)\t0.3565798233381452\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"I come to China to travel\", \n",
    "    \"This is a car polupar in China\",          \n",
    "    \"I love tea and Apple \",   \n",
    "    \"The work is to write some papers in science\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t0.4424621378947393\n",
      "  (0, 15)\t0.697684463383976\n",
      "  (0, 3)\t0.348842231691988\n",
      "  (0, 16)\t0.4424621378947393\n",
      "  (1, 3)\t0.3574550433419527\n",
      "  (1, 14)\t0.45338639737285463\n",
      "  (1, 6)\t0.3574550433419527\n",
      "  (1, 2)\t0.45338639737285463\n",
      "  (1, 9)\t0.45338639737285463\n",
      "  (1, 5)\t0.3574550433419527\n",
      "  (2, 7)\t0.5\n",
      "  (2, 12)\t0.5\n",
      "  (2, 0)\t0.5\n",
      "  (2, 1)\t0.5\n",
      "  (3, 15)\t0.2811316284405006\n",
      "  (3, 6)\t0.2811316284405006\n",
      "  (3, 5)\t0.2811316284405006\n",
      "  (3, 13)\t0.3565798233381452\n",
      "  (3, 17)\t0.3565798233381452\n",
      "  (3, 18)\t0.3565798233381452\n",
      "  (3, 11)\t0.3565798233381452\n",
      "  (3, 8)\t0.3565798233381452\n",
      "  (3, 10)\t0.3565798233381452\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf2 = TfidfVectorizer()\n",
    "re = tfidf2.fit_transform(corpus)\n",
    "print(re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出的各个文本各个词的TF-IDF值和第一种的输出完全相同。大家可以自己去验证一下。\n",
    "\n",
    "由于第二种方法比较的简洁，因此在实际应用中推荐使用，一步到位完成向量化，TF-IDF与标准化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF是非常常用的文本挖掘预处理基本步骤，但是如果预处理中使用了Hash Trick，则一般就无法使用TF-IDF了，因为Hash Trick后我们已经无法得到哈希后的各特征的IDF的值。使用了IF-IDF并标准化以后，我们就可以使用各个文本的词特征向量作为文本的特征，进行分类或者聚类分析。\n",
    "\n",
    "当然TF-IDF不光可以用于文本挖掘，在信息检索等很多领域都有使用。因此值得好好的理解这个方法的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**还的好好理解下 TF-IDF 是怎么实现的！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立分析模型\n",
    "有了每段文本的TF-IDF的特征向量，我们就可以利用这些数据建立分类模型，或者聚类模型了，或者进行主题模型的分析。比如我们上面的两段文本，就可以是两个训练样本了。此时的分类聚类模型和之前讲的非自然语言处理的数据分析没有什么两样。因此对应的算法都可以直接使用。而 **主题模型** 是自然语言处理比较特殊的一块，这个我们后面再单独讲。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文文本挖掘预处理总结\n",
    "上面我们对中文文本挖掘预处理的过程做了一个总结，希望可以帮助到大家。需要注意的是这个流程主要针对一些常用的文本挖掘，并使用了词袋模型，对于某一些自然语言处理的需求则流程需要修改。比如我们涉及到词上下文关系的一些需求，此时不能使用词袋模型。而有时候我们对于特征的处理有自己的特殊需求，因此这个流程仅供自然语言处理入门者参考。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码实现 \n",
    "#### CountVectorizer 类\n",
    "```python\n",
    "class CountVectorizer(BaseEstimator, VectorizerMixin):\n",
    "    \"\"\"Convert a collection of text documents to a matrix of token counts\n",
    "    This implementation produces a sparse representation of the counts using\n",
    "    scipy.sparse.csr_matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None,\n",
    "                 lowercase=True, preprocessor=None, tokenizer=None,\n",
    "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 ngram_range=(1, 1), analyzer='word',\n",
    "                 max_df=1.0, min_df=1, max_features=None,\n",
    "                 vocabulary=None, binary=False, dtype=np.int64):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    - input\n",
    "    \"\"\"\n",
    "```\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 18)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 9)\t1\n",
      "  (3, 25)\t1\n",
      "  (3, 22)\t1\n",
      "  (3, 13)\t1\n",
      "  (3, 24)\t1\n",
      "  (3, 21)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 12)\t1\n",
      "  (3, 23)\t1\n",
      "  (3, 20)\t1\n",
      "[[0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1]]\n",
      "feature names: ['apple', 'car', 'car polupar', 'car polupar china', 'china', 'china travel', 'come', 'come china', 'come china travel', 'love', 'love tea', 'love tea apple', 'papers', 'papers science', 'polupar', 'polupar china', 'science', 'tea', 'tea apple', 'travel', 'work', 'work write', 'work write papers', 'write', 'write papers', 'write papers science']\n",
      "\n",
      "stop words: frozenset({'although', 'seems', 'should', 'none', 'for', 'being', 'also', 'can', 'cannot', 'noone', 'amongst', 'keep', 'twelve', 'own', 'almost', 'beside', 'us', 'myself', 'whether', 'fill', 'are', 'become', 'might', 'ltd', 'be', 'more', 'towards', 'four', 'down', 'her', 'his', 'well', 'everywhere', 'hasnt', 'seem', 'seeming', 'inc', 'indeed', 'thereupon', 'third', 'together', 'else', 'itself', 'less', 'still', 'thin', 'ourselves', 'someone', 'alone', 'over', 'than', 'they', 'without', 'done', 'yourselves', 'becoming', 'both', 'first', 'made', 'call', 'namely', 'very', 'nine', 'were', 'hereafter', 'co', 'with', 'himself', 'below', 'out', 'mine', 'sometime', 'somewhere', 'off', 'there', 'became', 'forty', 'thick', 'during', 'where', 'how', 'due', 'yet', 'seemed', 'thru', 'sometimes', 'here', 'most', 'their', 'ten', 'whereupon', 'not', 'serious', 'get', 'therefore', 'who', 'latter', 'many', 'some', 'ever', 'around', 'everyone', 'fifty', 'thence', 'toward', 'etc', 'across', 'again', 'besides', 'everything', 'such', 'those', 'whom', 'whole', 'your', 'mill', 'beforehand', 'either', 'go', 'rather', 'though', 'why', 'formerly', 'twenty', 'at', 'our', 'part', 'which', 'move', 'ie', 'whoever', 'thereafter', 'sixty', 'whence', 'a', 'if', 'all', 'anyhow', 'empty', 'an', 'nowhere', 'or', 'fire', 'up', 'while', 'in', 'hereby', 'last', 'somehow', 'therein', 'system', 'perhaps', 'whereby', 'another', 'that', 'hers', 'themselves', 'this', 'after', 'neither', 'one', 'cant', 'ours', 'eight', 'something', 'between', 'however', 'sincere', 'the', 'next', 'see', 'i', 'take', 'about', 'amoungst', 'once', 'few', 'bill', 'my', 'put', 'un', 'same', 'whereas', 'so', 'thus', 'two', 'no', 'enough', 'con', 'often', 'other', 'hence', 'couldnt', 'since', 'top', 'give', 'was', 'anywhere', 'when', 'even', 'detail', 'within', 're', 'until', 'you', 'nevertheless', 'much', 'least', 'anyone', 'amount', 'nothing', 'front', 'further', 'may', 'onto', 'these', 'herein', 'bottom', 'latterly', 'anyway', 'wherever', 'now', 'and', 'whatever', 'afterwards', 'am', 'moreover', 'fifteen', 'as', 'five', 'except', 'behind', 'hundred', 'nor', 'we', 'he', 'former', 'do', 'me', 'others', 'full', 'whither', 'side', 'beyond', 'find', 'him', 'found', 'meanwhile', 'its', 'elsewhere', 'thereby', 'wherein', 'any', 'she', 'what', 'would', 'by', 'to', 'already', 'each', 'every', 'it', 'above', 'name', 'per', 'whereafter', 'had', 'of', 'on', 'back', 'always', 'yours', 'hereupon', 'show', 'cry', 'must', 'only', 'will', 'upon', 'anything', 'before', 'then', 'from', 'through', 'against', 'whose', 'eg', 'de', 'but', 'along', 'been', 'too', 'under', 'has', 'among', 'herself', 'interest', 'via', 'never', 'eleven', 'please', 'six', 'three', 'because', 'throughout', 'yourself', 'otherwise', 'into', 'whenever', 'have', 'becomes', 'could', 'mostly', 'them', 'describe', 'nobody', 'several', 'is'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\"I come to China to travel\", \n",
    "    \"This is a car polupar in China\",          \n",
    "    \"I love tea and Apple \",   \n",
    "    \"The work is to write some papers in science\"] \n",
    "vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1,3))\n",
    "text_vec = vectorizer.fit_transform(corpus)\n",
    "print(text_vec)\n",
    "print(text_vec.toarray())\n",
    "print(\"feature names:\",vectorizer.get_feature_names())\n",
    "print(\"\\nstop words:\",vectorizer.get_stop_words()) # 为什么这里没有打印出听用词? I a 应该都是吧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer 类\n",
    "```python\n",
    "class TfidfVectorizer(CountVectorizer):\n",
    "    \"\"\"Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "    Equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None, lowercase=True,\n",
    "                 preprocessor=None, tokenizer=None, analyzer='word',\n",
    "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 ngram_range=(1, 1), max_df=1.0, min_df=1,\n",
    "                 max_features=None, vocabulary=None, binary=False,\n",
    "                 dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,\n",
    "                 sublinear_tf=False):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer2 = TfidfVectorizer(ngram_range=(1,2),stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'car', 'car polupar', 'china', 'china travel', 'come', 'come china', 'love', 'love tea', 'papers', 'papers science', 'polupar', 'polupar china', 'science', 'tea', 'tea apple', 'travel', 'work', 'work write', 'write', 'write papers']\n"
     ]
    }
   ],
   "source": [
    "cont_vec = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 21)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_vec.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\" 沙 瑞金 叹 易学 赞习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易 学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易 学习 被 降职 到 道口 县当 县长 ， 王 大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王 大路 ， 就 和 易 学习 一起 给 王 大路 凑 了 5 万块 钱 ， 王 大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王 大路 竟然 做 得 风生水 起 。 沙 瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。 \" ,', '\" 沙 瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王 大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王 大路 就 在 京州帝 豪园 买 了 三套 别墅 ， 可是 李达 康和易 学习 都 不要 ， 这些 房子 都 在 王 大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。 \"']\n"
     ]
    }
   ],
   "source": [
    "### 中文处理\n",
    "import jieba\n",
    "content = []\n",
    "with open(\"./nlp_test1.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = jieba.cut(line.strip())\n",
    "        content.append(\" \".join(line))\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer3 = TfidfVectorizer()\n",
    "# raw_documents : iterable, an iterable which yields either str, unicode or file objects\n",
    "content_vec = vectorizer3.fit_transform(raw_documents=content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一起', '万块', '三人', '三套', '下海经商', '不想', '不要', '东挪西撮', '之后', '事业有成', '事对', '京州', '京州帝', '他们', '以沫', '公司', '分开', '别墅', '前一晚', '县当', '县长', '可是', '名下', '后来', '喝酒', '回忆起', '困难', '大家', '大路', '太大', '她们', '好像', '学习', '家住', '容易', '对不起', '康和易', '开始', '很大', '房子', '打听', '时期', '易学', '有福', '李达', '李达康', '欧阳', '毛娅', '没想到', '没有', '浪费', '瑞金', '百姓', '相助', '竟然', '股权', '胸怀', '自己', '觉得', '触动', '话别', '豪园', '赔礼道歉', '赞习', '踏实', '这些', '这件', '连连', '道口', '金山', '降职', '风生水']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer3.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 72)\n"
     ]
    }
   ],
   "source": [
    "print(content_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
